# Probabilistic Model-Based Clustering Methods

## Key Phrases/Concepts

- Probabilistic model, generative process.
- Probabilistic model-based clustering.
- Mixture model, Gausian mixture, Bernoulli mixture.
- Expectation-Maximization (EM), maximum likelihood.

## Probabilistic Model-Based Clustering Methods

- Probabilistic model
    - Model is data from generative process
    - Assume data is generated by mixture of underlying probability distributions
    - Attempt to optimize fit between observed data and mathematical model
- Probabilistic model based clustering
    - Each cluster can be represented by parametric probability distribution (Gaussian or Poisson)
    - Cluster is data points that most likely belong to same distribution
    - Cluster is parameter estimation to find maximum likelihood fit
- Types of model based clustering methods
    - Mixture models
        - Assume observations to be clusters and drawn from one of several components
        - Infer the parameters of these components
    - Expectation maximization (EM algorithm)
        - General technique to find maximum likelihood estimations in mixture models
    - Probabilistic topic models
        - PLSA - probabilistic latent semantic analysis
        - LDA - latent dirichlet allocation

## Mixture Model for Cluster Analysis

- Model based clustering
    - Task - find a set C of k probabilistic clusters sos that P(D|C) is maximized
        - Maximizing P(D|C) is often intractable since probability density function can take arbitrary complicated form
        - To max computationally feasible assume probability density functions are some parameterized distributions
- Parametric mixed models
    - Task - infer a set of K probabilistic clusters that is most likely to generate D
    - Each cluster is mathematically represented by parametric distribution
    - Mixture can be constructed with any type of components

## Gaussian Mixture Models

- We assume each cluster C is characterized by a multivariate normal distribution

![Untitled](Probabilistic%20Model-Based%20Clustering%20Methods%203712c86e7cd44a5bbce8f9bee3d4d883/Untitled.png)

- Maximum likelihood estimation (MLE) - directly maximizing his hard so we used EM approach
    - Expectation step - given current estimates compute the cluster posterior probability via Bayes
    - Re-estimate for each cluster

## Expectation Maximization (EM) Algorithm - Univariate

- K-means algorithm has two steps
    - Expectation step - given current cluster centers each object is assigned to cluster whose center is closest to the object (an object is expected to belong to the closest center)
    - Maximization step - given cluster assignment, adjust the center for each cluster so the sum of distance from all objects assigned to new center is minimized
- EM algorithm - framework to approach max likelihood or max posteriori estimates of parameters
    - E-step - assigns objects to clusters according to current parameters of probabilistic clusters
    - M-step - finds new clustering or parameters that minimize sum os square errors (SEE) or expected likelihood
- Algorithm
    - Initialization - randomly initialize cluster parameters
    - Execution - calculate posterior probability
    - Maximization - compute the maximum likelihood estimates of the cluster parameters

## Expectation Maximization (EM) Algorithm - Multivariate

- See lecture video

## Analysis of Mixture Model Methods

- K-means can be considered as a special case of EM
    - K-means is like a hard-EM where in the E-step we take local minimum instead of a distribution
    
    ![Untitled](Probabilistic%20Model-Based%20Clustering%20Methods%203712c86e7cd44a5bbce8f9bee3d4d883/Untitled%201.png)
    
- Gaussian mixture model (GMM) is the soft version of k-means
    - Calculate the distribution instead of most likely one in the E-step and use weighted sum to compute new centers in M-step
- Hard vs soft clustering assignments
    - K-means - hard; each point can belong to only one cluster
    - Probabilistic clustering - soft assignment of points to clusters
- Compared to K-means the EM algorithm for Gaussian mixture takes many more iterations to converge
- Strengths of mixture models
    - Are more general than partitioning and fuzzy clustering
    - Clusters can be characterized by a small number of parameters
    - Results may satisfy statistical assumptions of generate models
- Weaknesses of mixture models
    - Converge to local optimal (can run multiple times with random initialization)
    - Computationally expensive if number of distributions is large or dataset contains few observed points
    - Need large data sets
    - Hard to estimate the number of clusters

## Quiz

- Which of the following statements are true about probabilistic model based clustering?
    - It attempts to fit the observed data with the model using a probabilistic approach
    - It assumes that the data are generated by a mixture of underlying probability distributions.
    - It models the data from a generative process.
- Which of the following assumptions are made by the Guassian mixture model
    - Each data point is generated independently.
    - The probability density function of each data point is a Gaussian mixture model over all k cluster normals.
    - Each cluster is characterized by a Gaussian distribution.
- Which of the following statements are true about EM algorithm
    - EM algorithm is sensitive to the starting parameters.
        - Number of clusters is input to the EM algorithm
        - EM algorithm is not guaranteed to find the global optimal solution. It can get stuck in local optima depending on the starting parameters.
        - EM maximizes the likelihood.
- As shown in the figure bellow. Consider the following 1-dimensional dataset: x1=0,x2=0.09,x3=0.18,x4=0.9,x5=1. We apply the EM algorithm to train a GMM model for clustering. After the k-th iteration of the EM algorithm, the means of the two Gaussian components are μ0=0.5,μ1=0.75
    - The log-likelihood of the data will increase
    - On the next M-step, μ0*μ*0 will decrease, μ1*μ*1 will increase.