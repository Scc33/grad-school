# Final Exam

## Quiz 1

- Data science can be divided into two parts
    - **Data curation and data analytics**
- What of the following best describe differences between data curation and data analytics
    - **Data analytics is concerned with efficient and accurate knowledge extraction while data curation is concerned with ensuring the data used are reliable, findable and useable.**
    - **Data analytics focuses on extracting knowledge from data while data curation focusing on managing data to support data analytics.**
- Our definition of data science
    - Management
    - communication
    - Creation
    - Analysis
- One of the conclusions of the NAS reports on workforce preparation
    - **Domain-specific knowledge**
    - **Librarianship and archival practice**
    - **Computer and information science**
- Active and archival. Stonebraker et all (2013) define data curation as including discover of data, cleaning and transformation, semantic integration, and deduplication. In this paper which perspectives do they focus on
    - **Active curation**
- Grey et al (2002) make which of the following arguments about curation of data
    - **During a project, data are best managed by the project team. However, in the long term data should reside in a long-term science archive.**
        - They argue that during a project, the science team has the most knowledge of the data but archiving requires a separate set of skills.
    - **Documentation, procedures, software, and operations logs should be part of a data publication.**
        - They emphasize that this additional information (which they call "metadata") is essential to understanding the data.
- Which of the following statements do Buneman et al (2008) make about curated databases
    - **The creation of curated databases requires significant effort and expertise.**
    - **Provenance information can be used to track the source of information in a curated database.**
    - **Since the information in curated databases is often updated, it is important to be able to cite and retrieve specific versions.**
- Data collections universe in NSB report on long-lived digital data collections
    - **Digital data and data collections**
    - **Data authors, managers, users, and scientists**
    - **Software, hardware and communications infrastructure**
        - Software, hardware and communications infrastructure play an essential role in data production, use, and communication.
- Data organization
    - **Documenting and maintaining metadata about schema changes.**
        - As an organization strategy or schema evolves, it is important to maintain documentation about changes.
    - **Identifying and applying relevant standards for syntax and semantics.**
    - **Selecting an appropriate data model and schema.**
- Which curatorial activity is primarily concerned with identifying the inputs, calculations, and other actions responsible for data values?
    - Provenance

### Quiz 2

- Data model refers to…
    - **A type of framework for representing information. For example, the relational model with attributes, tuples, and values.**
    - A particular framework for representing information. For example, an XML schema such as
    - **The application of a particular framework or schema to represent information**
- Hiding the underlying details of storage and processing while exposing the essential features of the data itself refers to
    - Abstraction
- Enabling access to an underlying stored data representation through a separate representation (such as the relational model) refers to
    - Indirection
- Which level of abstraction focuses on modeling the data itself
    - Logical
- Abstractions at the logical level
    - Relations, trees, triples
- Codd's data independence principle
    - **The separation of the logical and physical levels in a database system**
    - **The independence of applications from underlying changes in data representation.**
- Specification of how data is or will be organized that may specify vocabulary, syntax, data types, attributes and value ranges
    - Schema
- Uses of integrity constraints
    - **Detecting logical inconsistencies in the data**
- Citation in evolving data bases
    - **Archiving of static exports of databases at a point in time**
    - **Storing data in a way that supports versioning and timestamping**
    - **Identifying datasets by assigning identifiers to timestamped queries**
- Which of the following is **not** a feature of the relational model?
    - **The ability to connect model attributes to the domain entities that they represent via ontologies.**

### Quiz 3

- Why are documents important for data curation
    - Documents are a natural unit of textual information.
    - A significant amount of information is represented using document-oriented formats (e.g., unstructured or semistructured)
    - Curation of documents requires an alternative modeling approach since other models, such as the relational model, are not well-suited for storing and representing documents.
- Which is **not** a characteristic of descriptive markup languages?
    - Through integrity constraints, they provide a rigorous framework for identifying logical inconsistencies.
- When we say the tree model and relational model address same underlying problems we're referring to
    - Storage and processing were intermixed resulting in a lack of data independence
    - Data (or text) were stored and represented in different ways
    - Explicit formal conceptualization of the data or text components was uncommon.
- Not a feature of descriptive markup model?
    - A model of constraints that can be used to enforce the relationships between objects in the hierarchy
- According to the lecture, how do abstractions in the “tree” model differ from abstractions in the relational model?
    - The relational model focuses on abstracting away from storage, while the tree model focuses on abstracting away from processing.
- Which is not a function of an XML schema?
    - Indicating access and modification privileges for the document instance.
- Which of the following is **not** required for an XML document to be well-formed?
    - It must conform to the grammar defined in an associated schema.
- An XML document instance is considered valid if
    - it is well-formed
    - it conform to the  grammar defined in an associated schema
- XML from database perspective. Which point is not made?
    - Like relational data, semistructured data have regular structures and strictly defined schemas.
- DDI (widely used standard for describing research data and underlies research data curation)
    - Data often cannot be understood or analyzed without accurate documentation. Providing a standard format for researchers to document data can reduce errors.
    - DDI was designed to capture both variable level (e.g., codebook) and study level documentation (e.g., study design, population, sampling procedures, etc).

### Quiz 4

- Both the relational and XML models lack
    - **the ability to specify real-world semantics**
- Describe the relationships between schemas and ontologies
    - **A schema defines the structure and organization of data while an ontology defines the semantics and relationships of concepts within a domain.**
- The logical basis for the ER model is important because
    - **it provides a specification of real world things, attributes, and relationships in a domain that can be used to give a logical schema meaning.**
- According to the “data model relationships” described in the lectures, at the conceptual level RDFS and OWL apply to which of the following logical models
    - Triples
- Chen makes which of the following points about the proposed entity-relationship model?
    - The entity-relationship model can be used to provide a unified view of data independent of the underlying model.
- Mapping between formal ontologies and logical schemas provides which of the following
    - Because the schemas now have semantics, instances have meaning and assert propositions.
    - A shared ontology can be used to identify common domain features across different schemas.
    - Mapping schemas to ontologies provides essential documentation.
    - **All of the above**
- If Codd encouraged abstraction away from storage methods to the data itself then Chen encourages us to
    - **abstract away from specific data models (e.g., tables, trees) to the information itself, the entities and relationships of the domain**
- Which of the following is **not** true about the RDF model as described in Decker, Mitra, Melnik (2000)
    - **In the RDF specification, the data model and syntax are both defined in XML.**
- In their overview of ontology engineer, Kendall and McGuinness state which of the following ontologies
    - The primary reason for creating an ontology is to make the meaning of terms, concepts, and relationships explicit for both humans and computers.
- Ensuring data quality is an essential part of data curation. In their discussion of RDF data quality and validation… Gayo state…
    - **Shape-based schema languages can be used to define a set of constraints to be applied to nodes in an RDF graph.**
    - **Given the same ontology and instance data, different applications require distinct validation constraints.**
- GO… which is **not**  part of the Gene Ontology Consortiums process
    - E. The GO uses a SHACL schema for validation

### Quiz 5

- What is data integration?
    - **Providing users with unified access to data from heterogenous sources.**
- Leaving data in their original format and mapping their schemas to a single global meta-schema is an example of:
    - Federation or mediation
- Different data description languages for the same model type, eg. RDF/XML vs N3, are an example of what kind of heterogeneity
    - Syntax heterogeneity
- Azzini et at. (2021) present a taxonomy of data integration architectures which of the following approaches do they not describe
    - **Integration whereby data are integrated based on distinct privacy and security frameworks.**
- Data et al. (2015) identify which of the following key challenges to data integration:
    - The same conceptual information may be modeled very differently across sources.
    - Instances may be represented differently across sources.
    - Instances may have ambiguities or inconsistencies across sources.
    - **All of the above**
- Terrizzano et al. (2015) describe the creation of research data lake at IBM. Which of the following points do they make about the data lake approach to data integration
    - The data lake approach postpones traditional data integration tasks, potentially to a time far removed from when the data was acquired.
- Ludascher et al (2006) describe a mediation framework for scientific data integration. Which of the following describes their approach?
    - A novel mediation approach that uses ontologies to integrate datasets based on domain-level semantics when schema-level integration is not viable.
- Based on Ilya's and Chus overview of data cleaning, which of the following is not one of the typical phases of a data cleaning workflow?
    - Error estimation
- According to Elias and Chu which of the following are commonly applied techniques for data cleaning?
    - Outlier detection
    - Entity resolution
    - Integrity constraints
    - Manual verification
    - **All of the above.**

### Quiz 6

- The identity problem described in lecture is central to the creation of a conceptual model for data. Which of the following is an example of the identity problem in data curation?
    - Determining whether a particular dataset already exists in an archive or catalogDetermining whether a particular dataset is the same one used in an analysisDetermining whether two datasets were derived from the same source
    - Determining whether two serializations contain the same data
    - **All of the above**
- According to the definition provided in lecture, which of the following can we say about data?
    - **Data are propositions asserted as evidence**
- According to Renear et al. (2010), which of the following is not true about the concept of a dataset?
    - **The concept of *dataset*  is problematic in common use because of the lack of a shared understanding**
- The FRBR model is relevant to the conceptual modeling of scientific data because
    - it defines entities and relationships that distinguish abstract intellectual content from different symbolic expressions, manifestations, and physical instances.
    - it provides a conceptual framework for the description, discovery, identification, and acquisition of a variety of objects (such as books, software, or even datasets) in bibliographic collections.
    - **Both of the above**
- Wickett et al. (2011) describe the Basic Representation Model (BRM) and Systematic Assertion Model (SAM). Which of the following is not a purpose of these models?
    - To address the issue of *scientific equivalence* of datasets
- The basic representation model (BRM) defines which of the following entities and relationships?
    - Propositional Content is *expressed* by Symbol structure. Symbol Structure can be *encoded* by Symbol Structure. Symbol Structure is *inscribed* in Patterned Matter and Energy.
- According to Times, Yesha, and Haley (2011) two datasets are scientifically equivalent if:
    - **they are sufficiently similar that their use in a scientific investigation would result in the same results or conclusions.**
- Brickly, Burgess, and Noy (2019) describe technical challenges implementing Google's dataset search. Which of the following is not one of the challenges discussed?
    - Traditional search engine ranking methods are equally effective with dataset search.
- Brickly, Burgess, and Noy (2019) describe which of the following methods for identifying duplicate datasets
    - Using the http://schema.org/sameAs property to explicitly identify when two datasets are the same.
    - Two datasets point to the same canonical page for a dataset regardless of whether the owner is using Schema.org metadata.
    - Computing hash value for combinations of values that may indicate a dataset is a duplicate.
    - **All of the above**

### Quiz 7

- Greenberg (2003), cited in lecture slides, defines metadata broadly as “structured data about an object that supports functions associated with the designated object.” According to lectures which of the following is not an example use of metadata?
    - **None of the above**
- According to lecture, a metadata schema is defined as “a set of data elements with specified meanings for supporting metadata statements in a particular context.” Which of the following is not a characteristic of metadata schemas discussed?
    - **Metadata schemas can be viewed as a conceptual model of a domain.**
- Riley (2017) provides a taxonomy of metadata types. Which of the following is not included in the taxonomy?
    - **Metadata about the quality of the resource**
- Neuman and Brase (2014) describe the DataCite initiative. Which of the following is not described by the authors?
    - **DataCite is also research data repository, providing storage and preservation services for scientific data.**
- Visengeriyeva and Abidjan (2020) present a unique perspective on metadata in the context of data quality and data cleaning. Which of the following do the authors discuss?
    - Quality information can be stored as useful metadata to better understand data.
    - Quality metadata can be acquired by profiling the original data
    - Quality metadata can be used to support data cleaning activities
    - **All of the above**
- Haley et al. (2016) describe the Google GOODS data catalog of billions of internal datasets. Which of the following types of metadata do they collect about the datasets?
    - **Basic information such as size, format, last modified time, and access control lists.**
    - **Content-based information such as the dataset's schema, key fields, and hashes/fingerprints of individual fields.**
    - **Provenance information about the dataset including how it was produced, how it is used, and whether other datasets depend on it.**
- Gebru et al. (2021) propose data sheets for datasets to facilitate communication between dataset creators and consumers. Which of the following points do the authors not make?
    - **The process of creating a datasheet is intended to be automated.**
- In general, a metadata schema (or scheme) defines a set of structured elements, along with their semantics and allowable values, to use to describe an object or resources. Which of the following is not an example of a metadata schema for the description of datasets or data catalogs?
    - **Hierarchical Data Format (HDF)**
- According to lecture, which of the following is not an example of core curatorial actives are supported by metadata?
    - **None of the above**

### Quiz 8

- According to lecture which of the following describe the empirical science of data curation:
    - The study of what people creating, analyzing, managing, and using data actually do.
    - An interdisciplinary field of study, but primarily based on social science research methods (i.e., the study of people and behaviors).
    - Addresses questions such as how we can more efficiently and reliably support the use of data.
    - **All of the above.**
- Based on the lectures, which of the following is described as a method generally used to study data practices?
    - Constructing an experiment or targeted data collection to confirm or refute a hypothesis.
    - Observing what people are actually doing in an ongoing data-related activity and using these observations to develop theories or ideas for interventions or changes in practice.
    - **Both of the above**
- Based on lecture, which of the following can be said about data sharing?
    - **Many researchers are concerned about the misinterpretation or misuse of their data if shared.**
- According to lecture which of the following is not a factoring affecting the reuse of data?
    - **None of the above**
- Tenopir et al. (2020) report the results of a study of data sharing attitudes and practices among researchers. Which of the following is not a key finding of their study:
    - **A majority of survey respondents use what the authors refer to as "good data practices" such as storing data in discipline or institutional research data repositories.**
- In a study of the long-term availability of data associated with publications, Federer (2022) analyzes a set of “Data Availability” statements extracted from articles published in the journal PLOS ONE. Which of the following is not a key finding of their study:
    - **Resources assigned DOIs or URLs were equally likely to be available**
- Stodden, Seiler, and Ma (2018) report on a study of the effectiveness of policies on the reproducibility of computational results reported in the journal Science, which had adopted a new policy required that authors make available data and code sufficient to support reproduction of results. Which of the following is not a finding or conclusion of their study?
    - **They conclude that the journal's policy is an effective method of ensuring the reproducibility of computational research.**
- Data practices research often involves the use of qualitative research methods. Karcher et al. (2016) describe the Qualitative Data Repository (QDR), a specialized research data repository for qualitative and mixed-methods research. Which of the following is not a challenge to qualitative data sharing discussed in the paper.
    - **None of the above**

### Quiz 9

- According to lecture, which of the following may prevent long-term access to and reuse of data?
    - Loss of contextual metadata
    - File format or software obsolescence
    - Human error or improper handling of data
    - Environmental conditions that may damage or destroy storage media.
    - **All of the above**
- Which of the following is not a cause of obsolescence discussed in lecture?
    - **Hardware failure**
- The emulation preservation strategy is to maintain software that can be used to emulate the original processing (we can also think of virtualization and containerization methods as part of this strategy). Which of the following is a drawback of the emulation strategy?
    - **The emulation or software environment must itself be preserved.**
- According to lecture, the preservation goal of *viability* refers to which of the following?
    - **Data can be correctly read from media**
- In lecture, we learned that preservation is concerned with ensuring the reliable communication of what is being asserted. Which of the following must be preserved?
    - **Neither of the above**
- According to lecture, the ability to verify that a particular dataset is what it claims to be refers to which of the following preservation goals?
    - **Authenticatable**
- With regard to the broader concept of digital preservation (not just data), Nadal (2018) notes that preservation activities focus on two broad classes of problems: obsolescence and damage. Which of the following preservation methods does **not** address the challenge of damage?
    - **Emulation**
- Which of the following is not a preservation strategy discussed in lecture?
    - **None of the above**
- According to CCDS (2012), the OAIS defines the concept of an information package that contains content information and preservation description information (PDI). Which of the following is not an example of a PDI type?
    - **The information that is the original target of preservation**
- Schiers et al. (2016, section 4) describe services for the preservation of scientific research offered at CERN, one of the world’s largest research centers. Which of the following services are described in the paper?
    - **Storage verification services such as regular validation of file metadata (e.g., sizes, checksums).**
    - **Hardware virtualization and containerization for the preservation of software stacks.**
    - **A system for the preservation of analysis "research objects" that include, for example, information about the data, software, configuration options, and links to documentation or publications.**

### Quiz 10

- According to lecture this week, identifiers enable which of the following?
    - Discovery and reuse of relevant datasets
    - Version control
    - Provenance tracking
    - Dataset citation
    - **All of the above**
- Assuming the RDF, JSON, and XML serializations contain the same assertions, the identifier (in this case a URL) refers to which of the following in the Basic Representation Model (BRM) levels discussed in lecture?
    - **Propositional content**
- Based the lecture on identifiers and change, which of the following identification schemes best reflects the immutability of digital objects?
    - **A file name based on the checksum or hash of the file contents**
- Based on the lectures, canonicalization is related to identification because:
    - Canonicalization is a technique for determining representational identity.
    - Canonicalization is a proxy for propositional identity.
    - Canonicalization can be used determine whether two data structures are logically equivalent.
    - **All of the above**
- Canonicalization applies only to XML
    - **False**
- Computing and storing a checksum or hash of an object's contents and using it determine whether the object has changed is an example of:
    - **Fixity**
- The Digital Preservation Handbook (Digital Preservation Coalition, 2017) discusses which of the following applications of fixity methods to detect unwanted changes:
    - Checking fixity on ingest.
    - Checking fixity of content held on preservation storage at regular intervals.
    - Checking fixity of content in response to specific activities, such as when accessed.
    - **All of the above**
- As with many formats, XML allows for documents that are logically equivalent to have different syntactic representations. The W3C Canonical XML recommendation describes a method for generating  a canonical representation that accounts for changes permitted by the XML standards. Which of the following statements is **not** true about Canonical XML?
    - **Two documents must have an identical canonical form to be equivalent**
- Which of the following is **not** a canonicalization step discussed in the W3C Canonical XML recommendation ?
    - **Remove whitespace in the document content**
- Duerr et al. (2011)  evaluate the  suitability of popular identifier systems against four use cases. For which of the following use cases did they find *low suitability* across all of the evaluated identifier systems?
    - **To be able to tell that two data instances contain the same information even in different formats**

## Quiz 11

- According to lecture, standards apply to the following data curation activities **except**:
    - **None of the above**
- According to lecture, which of the following may impede the *adoption* of a standard:
    - **Perceptions of partiality**
- According to lecture, a principle obstacle to *participation* in the standardization process is:
    - **Time and experience needed to master complex material**
- According to lecture, while standards are often seen as beneficial, adopting standard may pose certain challenges. Which of the following is **not** a challenge discussed in lecture?
    - **None of the above**
- Given two versions of a data standard **S1** and **S2**, ensuring that the largest possible number of datasets remain valid under both standards when complete compatibility is not possible is referred to in lecture as:
    - **P-compatability**
- Use of data and information standards often requires the use of software libraries or toolkits that implement the standard (e.g., rdflib to process RDF data). According to lecture, ensuring that a particular tool or library supports a standard (or features of a standard) refers to:
    - **Processor conformance**
- Based on the discussion in Harcourt et al. (2019), which of the following is **not** true about the W3C?
    - **Standards work is led by a diverse group of organizations.**
- In their discussion of the standards bodies, Harcourt et al. (2019) mention which of the following as a common characteristic of all of the reviewed standards bodies?
    - **Reliance on census**
- Rumble et al. (2020) discuss levels of formality in standards development. Which of the following is **not** an example of a level of formality discussed by the authors?
    - **None of the above**
- The W3C is responsible for the following standards or standards recommendations **except**:
    - **JSON**

## Quiz 12

- Which of the following is **not** an example of a data workflow?
    - None of the above
- Based on the lectures, which of the following is an example of a data transformation where input and output datasets have identical propositional content?
    - Transforming an input dataset from XML to JSON with no information loss
- Which of the following is defined as "a record of the origin and processing history of a data product or computational result"?
    - Computational provenance
- In the context of workflows, tasks are described having *black-box*, *white-box*, or *grey-box* provenance. Which of the following is an example of *black-box* provenance?
    - A task where underlying information about the inputs, outputs or processes is not available.
- According to the lecture and Ludaescher (2016) reading, which of the following describe the relationship between workflows and provenance?
    - The specification for a workflow is itself a form of prospective provenance.
    - Information captured about the execution of a workflow, such as the execution traces or logs, is a form of retrospective provenance.
    - **Both of the above**
- Which of the following is an example of *retrospective provenance*?
    - A detailed record or trace of the prior execution of a workflow.
- According to the lecture as well as the Ludaescher (2016) and Herschel et al. (2017) readings, provenance supports which of the following?
    - Explaining or making transparent processes used to obtain results.
    - Providing  information about a workflow to enable checking whether a  result can be reproduced.
    - Providing information about a workflow to enable assessment of data quality.
    - **All of the above**
- According to Ludaescher (2016), a scientific workflow is a description of a process for accomplishing an objective,  expressed in terms of tasks and their dependencies. Ludaescher highlights which of the following as a *key advantage* of using a workflow system?
    - Automated tracking of provenance
- According to Stodden et al. (2016), which of the following is **not** one of the  components recommended to support reproducibility of computational results?
    - None of the above
- According to Herschel et al. (2017), which of the following is the most specific type of provenance that requires the highest degree of process instrumentation?
    - Data provenance

## Quiz 13

- Which of the following is **not** an example of a *regulatory* policy related to data management and curation?
    - NIH Data sharing policy
- Which of the following is **not** considered protected information under the HIPAA Privacy Rule?
    - None of the above
- According to the lectures, complying with data privacy regulations may require which of the following?
    - Defining a strategy for managing and protecting sensitive data
    - Designing and implementing secure environments
    - Providing a system for and establishing access rights to protected data
    - **All of the above**
- Which of the following is **not** an example of an exhortative policy related to data management and curation?
    - US copyright law
- "Any information in the medical record or designated record set that can be used to identify an individual and that was created, used, or disclosed in the course of providing a health care service such as diagnosis or treatment" refers to which of the following?
    - Personal health information
- As discussed in lecture, data work often involves compliance with regulations, policies, and guidelines at multiple organizational levels. Which of the following is **not** an example of a regulation, policy or guideline that may affect data management and curation activities?
    - None of the above
- Carroll (2015) discusses how intellectual property law applies to research data. Which of the following is **not** true about data and intellectual property law?
    - Copyright protection applies to raw data and the original authors(s) maintain exclusive rights.
- *Informed consent* is central to the Federal Policy for the Protection of Human Subjects (aka the "Common Rule") as defined in the Belmont Report.  Informed consent refers to which of the following?
    - **Providing potential participants in research with sufficient information and the opportunity to choose whether to participate or not.**
- Refer to the OCR (2012) reading. Consider the example of a dataset with an "age" variable that needs to be transformed to reduce the risk of re-identifying subjects (to preserve privacy).  A method that randomly changes the age +/- 3 years is referred to as:
    - Perturbation
- Refer to OCR (2012). The HIPAA Privacy Rule defines acceptable methods for the de-identification of protected health information.  The "Safe Harbor" method refers to:
    - Removal or suppression of specified identifiers

### Quiz 15

- According to lecture, the central cause of the crises in scientific communication is:
    - Increased volume of published scientific papers
- According to the definition from Week 1 (and repeated this week), *data science* is **not** concerned with which of the following aspects of data?
    - None of the above
- According to lecture, the results of research activities are primarily recognized through which of the following activities?
    - Dissemination of findings
- According to Tenopir & King's findings reported in the lecture, which of the following is a reported finding about scientific reading?
    - Reading time per article is decreasing
- Which of the following is not an example of strategic reading
    - Reading individual articles in their entirety
- In their paper, Renear & Palmer (2009) predict which of the following with respect to scientific communication?
    - Enhanced search tools
    - Shared databases of annotations
    - Ontological inferencing
    - **All of the above**
- In their paper on strategic reading, Renear & Palmer (2009) describe which of the following developments in scientific publishing?
    - Integration of ontologies into scientific publishing workflow
    - Convergence on a single semi-structured schema for representation of scientific articles
    - **Both of the above**
- A central motivation for the Geoscience Paper of the Future (GPF) described in Gil et al. (2019) is:
    - Traditional scientific publications do not fully capture computational research data and methods.
- Which of the following is **not** a recommendation for the Geoscience Paper of the Future (GPF) described in Gil et al. (2019)?
    - None of the above
- Grimaldi & Ehler (2023) discuss the potential role of AI in scientific publishing. Which of the following is a risk of AI discussed in the article?
    - It can be used to generate scientific-looking papers

### Final Review Quiz

| Ensuring that data can be efficiently and reliably found and used | Data curation​ |
| --- | --- |
| Employing specific techniques to extract knowledge from data | Data analytics​ |
- Which of the following is **not** a level of abstraction for data models discussed in class?
    - Canonical
- Header —> Attribute
- Table entry —> Value
- Parenthesis —> Table
- Table name —> Relation
- Why do we need ontologies when the relational model already provides data independence by abstracting away from storage? (Select all that apply)
    - Ontologies abstract further, mapping data to things, properties, and relationships
    - Ontologies allow logical representations to be varied as needed
    - Ontologies support design and documentation independent of logical representation

| Data can be read from media | Viable​ |
| --- | --- |
| Data an be correctly viewed, processed, executed | Renderable​ |
| Data can be correctly determined to be what it purports to be | Authenticatable​ |
- RDF
    - Subject —> Predicate —> Object
- In short, the purpose of data curation is to make data analytics (select all that apply):
    - Efficient
    - Reliable
- *White-box provenance* refers to:
    - A mathematically exact representation of data and algorithms used is available
- What kind of data structure best describes an XML document?
    - Tree
- Given two datasets that are to be integrated, if both datasets use a completely different vocabulary to refer to the same concepts, we refer to this as which of the following?
    - Semantic heterogeneity