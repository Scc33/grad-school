# Week 1 - Intro to Data Cleaning

<aside>
ðŸ’¡ Goal

- Be familiar with the role of data cleaning in the larger data management and analysis life cycle.
- Spot certain data quality issues and associate them with a quality dimension and possibly a tool or approach that could help improve the quality of data.
- Know whether you should continue cleaning data or not, depending on the questions you're trying to answer with the data - "fitness for use."
</aside>

<aside>
ðŸ’¡ Questions to answer

- What is data cleaning or data wrangling?
- What is data quality and why is it important?
- What kinds of data errors can we distinguish?
- How can we improve data quality?
- What are common phases or steps in data quality management?
- How do we know when we're "done" with data cleaning?
</aside>

- **Data wrangling**
    - data processing that allows meaningful analysis to begin
        - Standardize, normalize, etc
    - extract, transform, load (ETL), integrate, clean, query, repair
        
        ![Screenshot 2023-05-16 at 10.40.55 PM.png](Week%201%20-%20Intro%20to%20Data%20Cleaning%20cc223ec0dcba4445bdfb255696c3c4a6/Screenshot_2023-05-16_at_10.40.55_PM.png)
        
    
    ![Untitled](Week%201%20-%20Intro%20to%20Data%20Cleaning%20cc223ec0dcba4445bdfb255696c3c4a6/Untitled.png)
    
- Data cleaning (wrangling)
    - Quality dimensions - data should be accurate, timely, relevant, completeâ€¦
    - Questions - is the quality sufficient?
    - Queries - data profiling, integrity constraints, etc (SQL)
- Data quality defined
    - Data is of high-quality if they are fit for use in operations, decision-making, planning etc
    - Fit for use when they are free of defects and posses features needed to complete operation, decisions, or complete the plan
    - Accuracy, completeness, consistency, timeliness
- Error types
    - Quantitative errors
        - Outliers - deviate significantly from distribution
    - Qualitative errors
        - Syntactic violations - ex spellings/patterns/format
        - Schema/integrity violations - functional or inclusion dependencies
        - Duplicates and other errors
- Pillars of data quality
    - Organization - objectives and strategies
    - Architectural - landscape and infrastructure to deploy DQ management
    - Computational - IT tools
- Common steps
    - Context reconstruction
    - Assessment and measurement
    - Improvement

### Quiz

Data cleaning or wrangling is generally part of a larger data quality management and analysis lifecycle.Â  According to Sadiq [Sad13b], which of the following have been key factors in recent changes to data quality management strategies?

- **Heterogeneity of data sources**
- **Volume of data**
    - Correct. The author identifies the volume of data produced by organizations as a factor in changes to data quality management.
- **Diversity of data types and formats**

According to Sadiq [Sad13b], the use of social media data by a researcher or another organization to draw conclusions or make decisions poses a potential quality concern because of which of the following:

- **The lack of alignment between why the data was created and how it is being used**
    - Correct. Sadiq highlights such lack of alignment as a factor in changes to data quality management.

Correct. Sadiq highlights such lack of alignment as a factor in changes to data quality management.

- **Techniques for record linkage and tracking data lineage provenance**
- **Techniques for ensuring semantic integrity constraints**

According to Ge and Helfert [GH13], examples of potential costs of relying on unimproved and low quality data in decision making include:

- **Loss of reputation**
- **Costs of wrong decisions or actions**

According to Ge and Helfert [GH13], if data is cleaned after collection, which of the following are potential costs:

- **Detection costs**
- **Repair costs**

According toÂ  Ilyas and Chu [IC19, Ch. 1], data deduplication involves determining that there are duplicates in the data, selecting methods for identifying duplicates, and consolidating duplicate records. Which steps of the data cleaning workflow are involved in deduplication?

- **Discovery or profiling**
- **Error detection**
- **Error repair**

A(n) ____ is an observation which deviates so much from other observations as to arouse suspicions that it was generated by a different mechanism.

- Outlier

According to Redman [Red13], *data quality* :

- **is inherently subjective and requires taking into account specific needs and applications**
- **is defined by fitness-for-use (having the right data and the data is free from defects)**
- **is made objective by defining specific quality dimensions**

According to Batini et al. [BCF+09], there are four data quality dimensions that are the focus of the majority of authors.  In their two-part study of data quality in a cancer registry, Bray & Parkin [[2009a](https://doi-org.proxy2.library.illinois.edu/10.1016/j.ejca.2008.11.032), [2009b](https://doi-org.proxy2.library.illinois.edu/10.1016/j.ejca.2008.11.033)], apply which of these dimensions:

- Accuracy
- Consistency
- Timeliness
- Completeness